/**
 * init command - Scaffold docker-compose.yml and config
 */

import { existsSync, mkdirSync, writeFileSync, readFileSync } from 'fs';
import { join } from 'path';
import { header, success, warn, info, dim, cyan, green, url, cmd, file, successBox } from '../output';
import { hasNvidiaGpu } from './doctor';

// =============================================================================
// Docker Compose Templates
// =============================================================================

/**
 * GPU-enabled Docker Compose template with NVIDIA runtime
 * Includes comprehensive documentation for each setting
 */
const DOCKER_COMPOSE_TEMPLATE_GPU = `# =============================================================================
# LOCLAUDE DOCKER COMPOSE - GPU MODE
# =============================================================================
# This configuration runs Ollama with NVIDIA GPU acceleration for fast inference.
# Generated by: loclaude init
#
# Prerequisites:
#   - NVIDIA GPU with CUDA support
#   - NVIDIA drivers installed on host
#   - NVIDIA Container Toolkit: https://docs.nvidia.com/datacenter/cloud-native/container-toolkit
#
# Quick test for GPU support:
#   docker run --rm --gpus all nvidia/cuda:12.0-base nvidia-smi
#
# =============================================================================

services:
  # ===========================================================================
  # OLLAMA - Local LLM Inference Server
  # ===========================================================================
  # Ollama provides the AI backend that Claude Code connects to.
  # It runs large language models locally on your hardware.
  #
  # API Documentation: https://github.com/ollama/ollama/blob/main/docs/api.md
  # Model Library: https://ollama.com/library
  # ===========================================================================
  ollama:
    # Official Ollama image - 'latest' ensures newest features and model support
    image: ollama/ollama:latest

    # Fixed container name for easy CLI access:
    #   docker exec ollama ollama list
    #   docker logs ollama
    container_name: ollama

    # NVIDIA Container Runtime - Required for GPU access
    # This makes CUDA libraries available inside the container
    runtime: nvidia

    environment:
      # ---------------------------------------------------------------------------
      # GPU Configuration
      # ---------------------------------------------------------------------------
      # NVIDIA_VISIBLE_DEVICES: Which GPUs to expose to the container
      #   - 'all': Use all available GPUs (recommended for most setups)
      #   - '0': Use only GPU 0
      #   - '0,1': Use GPUs 0 and 1
      - NVIDIA_VISIBLE_DEVICES=all

      # NVIDIA_DRIVER_CAPABILITIES: What GPU features to enable
      #   - 'compute': CUDA compute (required for inference)
      #   - 'utility': nvidia-smi and other tools
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility

      # ---------------------------------------------------------------------------
      # Ollama Configuration (Optional)
      # ---------------------------------------------------------------------------
      # Uncomment these to customize Ollama behavior:

      # Maximum number of models loaded in memory simultaneously
      # Lower this if you're running out of VRAM
      # - OLLAMA_MAX_LOADED_MODELS=1

      # Maximum parallel inference requests per model
      # Higher values use more VRAM but handle more concurrent requests
      # - OLLAMA_NUM_PARALLEL=1

      # Enable debug logging for troubleshooting
      # - OLLAMA_DEBUG=1

      # Custom model storage location (inside container)
      # - OLLAMA_MODELS=/root/.ollama

    volumes:
      # ---------------------------------------------------------------------------
      # Model Storage
      # ---------------------------------------------------------------------------
      # Maps ./models on your host to /root/.ollama in the container
      # This persists downloaded models across container restarts
      #
      # Disk space requirements (approximate):
      #   - 7B model:  ~4GB
      #   - 13B model: ~8GB
      #   - 30B model: ~16GB
      #   - 70B model: ~40GB
      - ./models:/root/.ollama

    ports:
      # Ollama API port - access at http://localhost:11434
      # Used by Claude Code and other Ollama clients
      - "11434:11434"

    # Restart policy - keeps Ollama running unless manually stopped
    restart: unless-stopped

    healthcheck:
      # Verify Ollama is responsive by listing models
      test: ["CMD", "ollama", "list"]
      interval: 300s      # Check every 5 minutes
      timeout: 2s         # Fail if no response in 2 seconds
      retries: 3          # Mark unhealthy after 3 consecutive failures
      start_period: 40s   # Grace period for initial model loading

    deploy:
      resources:
        reservations:
          devices:
            # Request GPU access from Docker
            - driver: nvidia
              count: all           # Use all available GPUs
              capabilities: [gpu]  # Request GPU compute capability

  # ===========================================================================
  # OPEN WEBUI - Chat Interface (Optional)
  # ===========================================================================
  # Open WebUI provides a ChatGPT-like interface for your local models.
  # Access at http://localhost:3000 after starting containers.
  #
  # Features:
  #   - Multi-model chat interface
  #   - Conversation history
  #   - Model management UI
  #   - RAG/document upload support
  #
  # Documentation: https://docs.openwebui.com/
  # ===========================================================================
  open-webui:
    # CUDA-enabled image for GPU-accelerated features (embeddings, etc.)
    # Change to :main if you don't need GPU features in the UI
    image: ghcr.io/open-webui/open-webui:cuda

    container_name: open-webui

    ports:
      # Web UI port - access at http://localhost:3000
      - "3000:8080"

    environment:
      # Tell Open WebUI where to find Ollama
      # Uses Docker internal networking (service name as hostname)
      - OLLAMA_BASE_URL=http://ollama:11434

    # Wait for Ollama to be ready before starting
    depends_on:
      - ollama

    restart: unless-stopped

    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

    volumes:
      # Persistent storage for conversations, settings, and user data
      - open-webui:/app/backend/data

    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

# =============================================================================
# VOLUMES
# =============================================================================
# Named volumes for persistent data that survives container recreation
volumes:
  open-webui:
    # Open WebUI data: conversations, user settings, uploads
    # Located at /var/lib/docker/volumes/open-webui/_data on host
`;

/**
 * CPU-only Docker Compose template without NVIDIA dependencies
 * For systems without NVIDIA GPUs or container toolkit
 */
const DOCKER_COMPOSE_TEMPLATE_CPU = `# =============================================================================
# LOCLAUDE DOCKER COMPOSE - CPU MODE
# =============================================================================
# This configuration runs Ollama in CPU-only mode.
# Inference will be slower than GPU mode but works on any system.
# Generated by: loclaude init --no-gpu
#
# Performance notes:
#   - 7B models: ~10-20 tokens/sec on modern CPUs
#   - Larger models will be significantly slower
#   - Consider using quantized models (Q4_K_M, Q5_K_M) for better performance
#
# Recommended CPU-optimized models:
#   - llama3.2:3b (fast, good for simple tasks)
#   - qwen2.5-coder:7b (coding tasks)
#   - gemma2:9b (general purpose)
#
# =============================================================================

services:
  # ===========================================================================
  # OLLAMA - Local LLM Inference Server (CPU Mode)
  # ===========================================================================
  # Ollama provides the AI backend that Claude Code connects to.
  # Running in CPU mode - no GPU acceleration.
  #
  # API Documentation: https://github.com/ollama/ollama/blob/main/docs/api.md
  # Model Library: https://ollama.com/library
  # ===========================================================================
  ollama:
    # Official Ollama image - works for both CPU and GPU
    image: ollama/ollama:latest

    # Fixed container name for easy CLI access
    container_name: ollama

    # NOTE: No 'runtime: nvidia' - running in CPU mode

    environment:
      # ---------------------------------------------------------------------------
      # Ollama Configuration (Optional)
      # ---------------------------------------------------------------------------
      # Uncomment these to customize Ollama behavior:

      # Maximum number of models loaded in memory simultaneously
      # CPU mode uses system RAM instead of VRAM
      # - OLLAMA_MAX_LOADED_MODELS=1

      # Number of CPU threads to use (default: auto-detect)
      # - OLLAMA_NUM_THREADS=8

      # Enable debug logging for troubleshooting
      # - OLLAMA_DEBUG=1

    volumes:
      # ---------------------------------------------------------------------------
      # Model Storage
      # ---------------------------------------------------------------------------
      # Maps ./models on your host to /root/.ollama in the container
      # This persists downloaded models across container restarts
      - ./models:/root/.ollama

    ports:
      # Ollama API port - access at http://localhost:11434
      - "11434:11434"

    restart: unless-stopped

    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 300s
      timeout: 2s
      retries: 3
      start_period: 40s

    # CPU resource limits (optional - uncomment to constrain)
    # deploy:
    #   resources:
    #     limits:
    #       cpus: '4'      # Limit to 4 CPU cores
    #       memory: 16G    # Limit to 16GB RAM
    #     reservations:
    #       cpus: '2'      # Reserve at least 2 cores
    #       memory: 8G     # Reserve at least 8GB RAM

  # ===========================================================================
  # OPEN WEBUI - Chat Interface (Optional)
  # ===========================================================================
  # Open WebUI provides a ChatGPT-like interface for your local models.
  # Access at http://localhost:3000 after starting containers.
  #
  # Documentation: https://docs.openwebui.com/
  # ===========================================================================
  open-webui:
    # Standard image (no CUDA) - smaller download, CPU-only features
    image: ghcr.io/open-webui/open-webui:main

    container_name: open-webui

    ports:
      - "3000:8080"

    environment:
      - OLLAMA_BASE_URL=http://ollama:11434

    depends_on:
      - ollama

    restart: unless-stopped

    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

    volumes:
      - open-webui:/app/backend/data

# =============================================================================
# VOLUMES
# =============================================================================
volumes:
  open-webui:
`;

// =============================================================================
// Configuration Templates
// =============================================================================

/**
 * Generate config template based on GPU mode
 */
function getConfigTemplate(gpu: boolean): string {
  return `{
  "ollama": {
    "url": "http://localhost:11434",
    "defaultModel": "${gpu ? 'qwen3-coder:30b' : 'qwen2.5-coder:7b'}"
  },
  "docker": {
    "composeFile": "./docker-compose.yml",
    "gpu": ${gpu}
  }
}
`;
}

// .gitignore template for models directory
const GITIGNORE_TEMPLATE = `# Ollama models (large binary files)
# These are downloaded by Ollama and can be re-pulled anytime
models/
`;

// mise.toml template with comprehensive task definitions
const MISE_TOML_TEMPLATE = `# =============================================================================
# MISE TASK RUNNER CONFIGURATION
# =============================================================================
# Mise is a task runner that provides convenient shortcuts for common operations.
# Run 'mise tasks' to see all available tasks.
#
# Documentation: https://mise.jdx.dev/
# Install: curl https://mise.jdx.dev/install.sh | sh
# =============================================================================

[tasks]

# =============================================================================
# Docker Management
# =============================================================================
# Commands for managing the Ollama and Open WebUI containers

[tasks.up]
description = "Start Ollama and Open WebUI containers"
run = "loclaude docker-up"

[tasks.down]
description = "Stop all containers"
run = "loclaude docker-down"

[tasks.restart]
description = "Restart all containers"
run = "loclaude docker-restart"

[tasks.status]
description = "Show container status"
run = "loclaude docker-status"

[tasks.logs]
description = "Follow container logs"
run = "loclaude docker-logs --follow"

# =============================================================================
# Model Management
# =============================================================================
# Commands for managing Ollama models (download, remove, list)

[tasks.models]
description = "List installed models"
run = "loclaude models"

[tasks.pull]
description = "Pull a model (usage: mise run pull <model-name>)"
run = "loclaude models-pull {{arg(name='model')}}"

[tasks."pull:recommended"]
description = "Pull the recommended coding model"
run = "loclaude models-pull qwen3-coder:30b"

# =============================================================================
# Claude Code
# =============================================================================
# Commands for running Claude Code with local Ollama

[tasks.claude]
description = "Run Claude Code with local Ollama"
run = "loclaude run"

[tasks."claude:model"]
description = "Run Claude with specific model (usage: mise run claude:model <model>)"
run = "loclaude run -m {{arg(name='model')}}"

# =============================================================================
# Diagnostics
# =============================================================================
# Commands for checking system health and troubleshooting

[tasks.doctor]
description = "Check system requirements"
run = "loclaude doctor"

[tasks.gpu]
description = "Check GPU status (requires NVIDIA GPU)"
run = "docker exec ollama nvidia-smi"

[tasks.config]
description = "Show current configuration"
run = "loclaude config"
`;

// README.md template with comprehensive documentation
const README_TEMPLATE = `# Project Name

> Powered by [loclaude](https://github.com/nicholasgalante1997/loclaude) - Run Claude Code with local Ollama LLMs

## Prerequisites

- [Docker](https://docs.docker.com/get-docker/) with Docker Compose v2
- [mise](https://mise.jdx.dev/) task runner (recommended)
- [loclaude](https://www.npmjs.com/package/loclaude) CLI (\`npm install -g loclaude\`)

### For GPU Mode (Recommended)

- [NVIDIA GPU](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html) with CUDA support
- NVIDIA drivers installed on host
- [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html)

## Quick Start

\`\`\`bash
# Start the LLM backend (Ollama + Open WebUI)
mise run up

# Pull a model (adjust based on your hardware)
mise run pull qwen3-coder:30b  # GPU: 30B model (~16GB VRAM)
mise run pull qwen2.5-coder:7b # CPU: 7B model (faster)

# Run Claude Code with local LLM
mise run claude
\`\`\`

## Available Commands

Run \`mise tasks\` to see all available commands.

| Command | Description |
|---------|-------------|
| \`mise run up\` | Start Ollama and Open WebUI containers |
| \`mise run down\` | Stop all containers |
| \`mise run status\` | Show container status |
| \`mise run logs\` | Follow container logs |
| \`mise run models\` | List installed models |
| \`mise run pull <model>\` | Pull a model from Ollama registry |
| \`mise run claude\` | Run Claude Code with model selection |
| \`mise run claude:model <model>\` | Run Claude with specific model |
| \`mise run doctor\` | Check system requirements |
| \`mise run gpu\` | Check GPU status |

## Service URLs

| Service | URL | Description |
|---------|-----|-------------|
| Ollama API | http://localhost:11434 | LLM inference API |
| Open WebUI | http://localhost:3000 | Chat interface |

## Project Structure

\`\`\`
.
├── .claude/
│   └── CLAUDE.md          # Claude Code project instructions
├── .loclaude/
│   └── config.json        # Loclaude configuration
├── models/                # Ollama model storage (gitignored)
├── docker-compose.yml     # Container definitions
├── mise.toml              # Task runner configuration
└── README.md
\`\`\`

## Configuration

### Loclaude Config (\`.loclaude/config.json\`)

\`\`\`json
{
  "ollama": {
    "url": "http://localhost:11434",
    "defaultModel": "qwen3-coder:30b"
  },
  "docker": {
    "composeFile": "./docker-compose.yml",
    "gpu": true
  }
}
\`\`\`

### Environment Variables

| Variable | Description | Default |
|----------|-------------|---------|
| \`OLLAMA_URL\` | Ollama API endpoint | \`http://localhost:11434\` |
| \`OLLAMA_MODEL\` | Default model name | \`qwen3-coder:30b\` |
| \`LOCLAUDE_GPU\` | Enable GPU mode | \`true\` |

## Recommended Models

### For GPU (NVIDIA with 16GB+ VRAM)

| Model | Size | Use Case |
|-------|------|----------|
| \`qwen3-coder:30b\` | ~16GB | Best coding performance |
| \`gpt-oss:20b\` | ~12GB | General purpose |
| \`glm-4.7:cloud\` | Cloud | No local storage needed |

### For CPU or Limited VRAM

| Model | Size | Use Case |
|-------|------|----------|
| \`qwen2.5-coder:7b\` | ~4GB | Coding on CPU |
| \`llama3.2:3b\` | ~2GB | Fast, simple tasks |
| \`gemma2:9b\` | ~5GB | General purpose |

## Troubleshooting

### Check System Requirements

\`\`\`bash
mise run doctor
\`\`\`

### View Container Logs

\`\`\`bash
mise run logs
\`\`\`

### Restart Containers

\`\`\`bash
mise run down && mise run up
\`\`\`

### GPU Not Detected

1. Verify NVIDIA drivers: \`nvidia-smi\`
2. Check Docker GPU access: \`docker run --rm --gpus all nvidia/cuda:12.0-base nvidia-smi\`
3. Install NVIDIA Container Toolkit if missing

## License

MIT
`;

// .claude/CLAUDE.md template
const CLAUDE_MD_TEMPLATE = `# Claude Code Instructions

Project-specific instructions for Claude Code.

## Project Overview

This project uses [loclaude](https://github.com/nicholasgalante1997/loclaude) to run Claude Code with local Ollama LLMs.

## Quick Reference

\`\`\`bash
# Start the LLM backend
mise run up

# Run Claude Code
mise run claude

# Check system status
mise run doctor
\`\`\`

## Available Commands

| Command | Description |
|---------|-------------|
| \`mise run up\` | Start Ollama + Open WebUI containers |
| \`mise run down\` | Stop containers |
| \`mise run claude\` | Run Claude Code with model selection |
| \`mise run models\` | List installed models |
| \`mise run pull <model>\` | Pull a new model |
| \`mise run doctor\` | Check prerequisites |

## Service URLs

- **Ollama API:** http://localhost:11434
- **Open WebUI:** http://localhost:3000

## Configuration

- **Docker:** \`docker-compose.yml\`
- **Loclaude:** \`.loclaude/config.json\`
- **Tasks:** \`mise.toml\`

## Conventions

<!-- Add project-specific conventions here -->

## Do Not

- Commit the \`models/\` directory (contains large model files)
`;

// =============================================================================
// Init Command
// =============================================================================

export interface InitOptions {
  force?: boolean;
  noWebui?: boolean;
  gpu?: boolean;
  noGpu?: boolean;
}

export async function init(options: InitOptions = {}): Promise<void> {
  const cwd = process.cwd();
  const composePath = join(cwd, 'docker-compose.yml');
  const configDir = join(cwd, '.loclaude');
  const configPath = join(configDir, 'config.json');
  const modelsDir = join(cwd, 'models');
  const gitignorePath = join(cwd, '.gitignore');
  const miseTomlPath = join(cwd, 'mise.toml');
  const claudeDir = join(cwd, '.claude');
  const claudeMdPath = join(claudeDir, 'CLAUDE.md');
  const readmePath = join(cwd, 'README.md');

  header('Initializing loclaude project');
  console.log('');

  // Determine GPU mode
  // Note: cac converts --no-gpu to gpu: false, and --gpu to gpu: true
  let gpuMode: boolean;
  if (options.gpu === false) {
    gpuMode = false;
    console.log(info('CPU-only mode (--no-gpu)'));
  } else if (options.gpu === true) {
    gpuMode = true;
    console.log(info('GPU mode enabled (--gpu)'));
  } else {
    // Auto-detect GPU
    console.log(dim('  Detecting GPU...'));
    gpuMode = await hasNvidiaGpu();
    if (gpuMode) {
      console.log(success('NVIDIA GPU detected - using GPU mode'));
    } else {
      console.log(warn('No NVIDIA GPU detected - using CPU mode'));
      console.log(dim('    Use --gpu to force GPU mode if you have an NVIDIA GPU'));
    }
  }
  console.log('');

  // Create README.md
  if (existsSync(readmePath) && !options.force) {
    console.log(warn(`${file('README.md')} already exists`));
  } else {
    writeFileSync(readmePath, README_TEMPLATE);
    console.log(success(`Created ${file('README.md')}`));
  }

  // Check for existing docker-compose.yml
  if (existsSync(composePath) && !options.force) {
    console.log(warn(`${file('docker-compose.yml')} already exists`));
    console.log(dim('    Use --force to overwrite'));
  } else {
    // Select appropriate template
    let composeContent = gpuMode ? DOCKER_COMPOSE_TEMPLATE_GPU : DOCKER_COMPOSE_TEMPLATE_CPU;

    // Remove open-webui if --no-webui flag
    if (options.noWebui) {
      // Remove the open-webui service and volume
      composeContent = composeContent
        .replace(/\n  # =+\n  # OPEN WEBUI[\s\S]*?capabilities: \[gpu\]\n/m, '\n')
        .replace(/\n  # =+\n  # OPEN WEBUI[\s\S]*?open-webui:\/app\/backend\/data\n/m, '\n')
        .replace(/\nvolumes:\n  open-webui:\n.*$/m, '\n');
    }

    writeFileSync(composePath, composeContent);
    const modeLabel = gpuMode ? cyan('GPU') : cyan('CPU');
    console.log(success(`Created ${file('docker-compose.yml')} (${modeLabel} mode)`));
  }

  // Create mise.toml
  if (existsSync(miseTomlPath) && !options.force) {
    console.log(warn(`${file('mise.toml')} already exists`));
  } else {
    writeFileSync(miseTomlPath, MISE_TOML_TEMPLATE);
    console.log(success(`Created ${file('mise.toml')}`));
  }

  // Create .claude directory and CLAUDE.md
  if (!existsSync(claudeDir)) {
    mkdirSync(claudeDir, { recursive: true });
  }
  if (existsSync(claudeMdPath) && !options.force) {
    console.log(warn(`${file('.claude/CLAUDE.md')} already exists`));
  } else {
    writeFileSync(claudeMdPath, CLAUDE_MD_TEMPLATE);
    console.log(success(`Created ${file('.claude/CLAUDE.md')}`));
  }

  // Create .loclaude config directory
  if (!existsSync(configDir)) {
    mkdirSync(configDir, { recursive: true });
    console.log(success(`Created ${file('.loclaude/')} directory`));
  }

  // Create config file
  if (existsSync(configPath) && !options.force) {
    console.log(warn(`${file('.loclaude/config.json')} already exists`));
  } else {
    writeFileSync(configPath, getConfigTemplate(gpuMode));
    console.log(success(`Created ${file('.loclaude/config.json')}`));
  }

  // Create models directory
  if (!existsSync(modelsDir)) {
    mkdirSync(modelsDir, { recursive: true });
    console.log(success(`Created ${file('models/')} directory`));
  }

  // Append to .gitignore if it exists, or create it
  if (existsSync(gitignorePath)) {
    const existing = readFileSync(gitignorePath, 'utf-8');
    if (!existing.includes('models/')) {
      writeFileSync(gitignorePath, existing + '\n' + GITIGNORE_TEMPLATE);
      console.log(success(`Updated ${file('.gitignore')}`));
    }
  } else {
    writeFileSync(gitignorePath, GITIGNORE_TEMPLATE);
    console.log(success(`Created ${file('.gitignore')}`));
  }

  // Print success summary
  const recommendedModel = gpuMode ? 'qwen3-coder:30b' : 'qwen2.5-coder:7b';

  console.log('');
  console.log(green('Project initialized!'));
  console.log('');
  console.log(cyan('Next steps:'));
  console.log(`  1. Start containers:  ${cmd('mise run up')}`);
  console.log(`  2. Pull a model:      ${cmd(`mise run pull ${recommendedModel}`)}`);
  console.log(`  3. Run Claude:        ${cmd('mise run claude')}`);
  console.log('');
  console.log(cyan('Service URLs:'));
  console.log(`  Ollama API:  ${url('http://localhost:11434')}`);
  if (!options.noWebui) {
    console.log(`  Open WebUI:  ${url('http://localhost:3000')}`);
  }
}

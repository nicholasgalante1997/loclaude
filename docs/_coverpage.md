![logo](_media/logo.svg ':size=120')

# loclaude

> Run Claude Code with local Ollama LLMs

- AI-powered coding assistant
- Local inference with Ollama
- Docker-ready setup
- Cross-runtime support

[Get Started](getting-started.md)
[GitHub](https://github.com/nicholasgalante1997/docker-ollama)

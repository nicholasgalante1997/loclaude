# =============================================================================
# LOCLAUDE DOCKER COMPOSE - GPU MODE
# =============================================================================
# This configuration runs Ollama with NVIDIA GPU acceleration for fast inference.
# Bundled with loclaude package for use as a fallback when no local compose exists.
#
# Prerequisites:
#   - NVIDIA GPU with CUDA support
#   - NVIDIA drivers installed on host
#   - NVIDIA Container Toolkit: https://docs.nvidia.com/datacenter/cloud-native/container-toolkit
#
# Quick test for GPU support:
#   docker run --rm --gpus all nvidia/cuda:12.0-base nvidia-smi
#
# =============================================================================

services:
  # ===========================================================================
  # OLLAMA - Local LLM Inference Server
  # ===========================================================================
  # Ollama provides the AI backend that Claude Code connects to.
  # It runs large language models locally on your hardware.
  #
  # API Documentation: https://github.com/ollama/ollama/blob/main/docs/api.md
  # Model Library: https://ollama.com/library
  # ===========================================================================
  ollama:
    # Official Ollama image - 'latest' ensures newest features and model support
    image: ollama/ollama:latest

    # Fixed container name for easy CLI access:
    #   docker exec ollama ollama list
    #   docker logs ollama
    container_name: ollama

    # NVIDIA Container Runtime - Required for GPU access
    # This makes CUDA libraries available inside the container
    runtime: nvidia

    environment:
      # ---------------------------------------------------------------------------
      # GPU Configuration
      # ---------------------------------------------------------------------------
      # NVIDIA_VISIBLE_DEVICES: Which GPUs to expose to the container
      #   - 'all': Use all available GPUs (recommended for most setups)
      #   - '0': Use only GPU 0
      #   - '0,1': Use GPUs 0 and 1
      - NVIDIA_VISIBLE_DEVICES=all

      # NVIDIA_DRIVER_CAPABILITIES: What GPU features to enable
      #   - 'compute': CUDA compute (required for inference)
      #   - 'utility': nvidia-smi and other tools
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility

      # ---------------------------------------------------------------------------
      # Ollama Configuration (Optional)
      # ---------------------------------------------------------------------------
      # Uncomment these to customize Ollama behavior:

      # Maximum number of models loaded in memory simultaneously
      # Lower this if you're running out of VRAM
      # - OLLAMA_MAX_LOADED_MODELS=1

      # Maximum parallel inference requests per model
      # Higher values use more VRAM but handle more concurrent requests
      # - OLLAMA_NUM_PARALLEL=1

      # Enable debug logging for troubleshooting
      # - OLLAMA_DEBUG=1

    volumes:
      # ---------------------------------------------------------------------------
      # Model Storage
      # ---------------------------------------------------------------------------
      # Maps ./models on your host to /root/.ollama in the container
      # This persists downloaded models across container restarts
      #
      # Disk space requirements (approximate):
      #   - 7B model:  ~4GB
      #   - 13B model: ~8GB
      #   - 30B model: ~16GB
      #   - 70B model: ~40GB
      - ./models:/root/.ollama

    ports:
      # Ollama API port - access at http://localhost:11434
      # Used by Claude Code and other Ollama clients
      - "11434:11434"

    # Restart policy - keeps Ollama running unless manually stopped
    restart: unless-stopped

    healthcheck:
      # Verify Ollama is responsive by listing models
      test: ["CMD", "ollama", "list"]
      interval: 300s      # Check every 5 minutes
      timeout: 2s         # Fail if no response in 2 seconds
      retries: 3          # Mark unhealthy after 3 consecutive failures
      start_period: 40s   # Grace period for initial model loading

    deploy:
      resources:
        reservations:
          devices:
            # Request GPU access from Docker
            - driver: nvidia
              count: all           # Use all available GPUs
              capabilities: [gpu]  # Request GPU compute capability

  # ===========================================================================
  # OPEN WEBUI - Chat Interface (Optional)
  # ===========================================================================
  # Open WebUI provides a ChatGPT-like interface for your local models.
  # Access at http://localhost:3000 after starting containers.
  #
  # Features:
  #   - Multi-model chat interface
  #   - Conversation history
  #   - Model management UI
  #   - RAG/document upload support
  #
  # Documentation: https://docs.openwebui.com/
  # ===========================================================================
  open-webui:
    # CUDA-enabled image for GPU-accelerated features (embeddings, etc.)
    # Change to :main if you don't need GPU features in the UI
    image: ghcr.io/open-webui/open-webui:cuda

    container_name: open-webui

    ports:
      # Web UI port - access at http://localhost:3000
      - "3000:8080"

    environment:
      # Tell Open WebUI where to find Ollama
      # Uses Docker internal networking (service name as hostname)
      - OLLAMA_BASE_URL=http://ollama:11434

    # Wait for Ollama to be ready before starting
    depends_on:
      - ollama

    restart: unless-stopped

    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

    volumes:
      # Persistent storage for conversations, settings, and user data
      - open-webui:/app/backend/data

    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

# =============================================================================
# VOLUMES
# =============================================================================
# Named volumes for persistent data that survives container recreation
volumes:
  open-webui:
    # Open WebUI data: conversations, user settings, uploads
    # Located at /var/lib/docker/volumes/open-webui/_data on host

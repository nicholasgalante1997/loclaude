services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    # Use nvidia runtime for GPU acceleration
    # This enables access to Nvidia GPUs from within the container
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all # Make all GPUs visible to the container
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility # Grant compute and utility capabilities (needed for GPU inference)
      # OPTIONAL: Set memory limits for Ollama process (in bytes)
      # Uncomment if you want to prevent Ollama from consuming unlimited RAM
      # - OLLAMA_MAX_LOADED_MODELS=1
      # - OLLAMA_NUM_PARALLEL=1
      
      # OPTIONAL: Set log level for debugging
      # - OLLAMA_DEBUG=1
    
    # Volume mounts: maps host directories/files into the container
    volumes:
      # Map the models directory so they persist on your host
      # Models downloaded in container go to /root/.ollama, we mount it to ./models on host
      - ./models:/root/.ollama
      
      # Keep container time in sync with host (good practice)
      # - /etc/localtime:/etc/localtime:ro
      
      # OPTIONAL: Mount a custom config directory
      # Uncomment if you want to customize Ollama settings
      # - ./config:/root/.ollama/config

    ports:
      - "11434:11434"
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 300s
      timeout: 2s
      retries: 3
      start_period: 40s
    
    # OPTIONAL: Resource limits and reservations
    # Uncomment to constrain CPU and memory usage
    deploy:
      resources:
        # limits:
        #   cpus: '4'           # Limit to 4 CPU cores
        #   memory: 32G         # Limit to 32GB RAM
        reservations:
          # cpus: '2'           # Reserve at least 2 CPU cores
          # memory: 16G         # Reserve at least 16GB RAM
          devices:
            - driver: nvidia
              count: all      # Use all available GPUs
              capabilities: [gpu]
  open-webui:
    image: ghcr.io/open-webui/open-webui:cuda # For Nvidia GPU support, you change the image from ghcr.io/open-webui/open-webui:main to ghcr.io/open-webui/open-webui:cuda
    container_name: open-webui
    ports:
      - "3000:8080"
    environment:
      # Point Open WebUI to the Ollama service
      # Use the service name (ollama) as the hostname since they're on the same Docker network
      - OLLAMA_BASE_URL=http://ollama:11434
    depends_on:
      - ollama # Ensure Ollama starts before Open WebUI
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    volumes:
      - open-webui:/app/backend/data
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

volumes:
  open-webui:
